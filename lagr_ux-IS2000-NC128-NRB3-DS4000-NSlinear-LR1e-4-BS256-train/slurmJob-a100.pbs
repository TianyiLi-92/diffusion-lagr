#!/bin/bash
#SBATCH -p std
#SBATCH --time 24:00:00      # format: HH:MM:SS
##SBATCH -w a100-2           # ask for specific host
#SBATCH -N 1                 # xxx node
#SBATCH --ntasks-per-node=16 # xxx core per node
#SBATCH --gres=gpu:a100:4    # xxx gpus per node
#SBATCH --mem=256000         # memory per node out of xxx MB
#SBATCH --job-name=lagr-train
##SBATCH --mail-type=ALL
##SBATCH --mail-user=<user_email>

echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

source ~/.bashrc
conda activate diffusion-lagr

export OPENAI_LOGDIR="/mnt/petaStor/li/Job/diffusion-lagr/lagr_ux-IS2000-NC128-NRB3-DS4000-NSlinear-LR1e-4-BS256-train"

DATA_FLAGS="--dataset_path /mnt/petaStor/li/Job/diffusion-lagr/datasets/Lagr_ux_diffusion.h5 --dataset_name train"
MODEL_FLAGS="--dims 1 --image_size 2000 --in_channels 1 --num_channels 128 --num_res_blocks 3 --attention_resolutions 250,125 --channel_mult 1,1,2,3,4"
DIFFUSION_FLAGS="--diffusion_steps 4000 --noise_schedule linear"
TRAIN_FLAGS="--lr 1e-4 --batch_size 64"

mpiexec -n 4 python ../scripts/turb_train.py $DATA_FLAGS $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS
